{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе. Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании вы:\n",
    "\n",
    "потренируетесь считать градиенты различных многомерных функций\n",
    "реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "реализуете процесс тренировки линейного классификатора\n",
    "подберете параметры тренировки на практике\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4659/3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/tmp/ipykernel_4659/3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "prediction is \n",
      " [3.]\n",
      "analytic grad is \n",
      " [6.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4659/3146107952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marray_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# derivative for it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mnumeric_grad_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x shape[1] \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==========================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "90\n",
      "5\n",
      "[[ True  True False  True]\n",
      " [ True False False False]]\n",
      "(1, 0)\n",
      "new a is  [[-2 -3 -1  0]\n",
      " [-2 -2 -3 -4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3,2,4,5], [3,3,2,1]])\n",
    "b = np.array([3,2,90,5])\n",
    "for elem in b:\n",
    "    print(elem)\n",
    "\n",
    "check = np.isclose(a, b, atol=0.5)\n",
    "print(check)\n",
    "\n",
    "\n",
    "it = np.nditer(a, flags=['multi_index'], op_flags=['readwrite'])\n",
    "ix = it.multi_index\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "# it.()\n",
    "print(it.multi_index)\n",
    "\n",
    "a -= np.max(a)\n",
    "print(\"new a is \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions \n",
      " [-10   0  10]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "orig predictions \n",
      " [1000    0    0]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions \n",
      " [-5  0  5]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[5.00676044]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.00676044]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4032/278634717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO Implement combined function or softmax and cross entropy and produces gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[0;34m(predictions, target_index)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     '''\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# print(\"probs are \", probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# print(\"orig predictions \\n\", orig_predictions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmax_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# print(\"max pred is \\n\", max_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_elem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     37\u001b[0m def _amax(a, axis=None, out=None, keepdims=False,\n\u001b[1;32m     38\u001b[0m           initial=_NoValue, where=True):\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m def _amin(a, axis=None, out=None, keepdims=False,\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([[1]]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([[1]])), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "prediction is \n",
      " [[ 1.  2. -1.  1.]]\n",
      "loss array \n",
      " [3.57972422]\n",
      "analytic grad is \n",
      " [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "x shape[1]  4\n",
      "==========================================\n",
      "loss array \n",
      " [3.57972628]\n",
      "loss array \n",
      " [3.57972216]\n",
      "loss array \n",
      " [3.57972982]\n",
      "loss array \n",
      " [3.57971862]\n",
      "loss array \n",
      " [3.5797145]\n",
      "loss array \n",
      " [3.57973394]\n",
      "loss array \n",
      " [3.57972628]\n",
      "loss array \n",
      " [3.57972216]\n",
      "numeric grad array is \n",
      " [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "Gradient check passed!\n",
      "END\n",
      "CHECK GRADIENT\n",
      "prediction is \n",
      " [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88280282]\n",
      "analytic grad is \n",
      " [[ 0.68145256  0.03392753  0.03392753 -0.74930761]\n",
      " [ 0.10923177  0.29692274  0.29692274 -0.70307726]\n",
      " [ 0.15216302  0.41362198 -0.97940697  0.41362198]]\n",
      "x shape[1]  4\n",
      "==========================================\n",
      "loss array \n",
      " [1.38353545 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38352182 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38352898 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.3835283  1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38352898 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.3835283  1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38352115 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38353613 1.2142833  3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428439 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428221 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428627 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428033 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428627 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21428033 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21427627 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.21429033 3.88280282]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88280434]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.8828013 ]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88280696]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88279869]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88279303]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88281262]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88280696]\n",
      "loss array \n",
      " [1.38352864 1.2142833  3.88279869]\n",
      "numeric grad array is \n",
      " [[ 0.22715085  0.01130918  0.01130918 -0.2497692 ]\n",
      " [ 0.03641059  0.09897425  0.09897425 -0.23435909]\n",
      " [ 0.05072101  0.13787399 -0.32646899  0.13787399]]\n",
      "Gradients are different at (0, 0). Analytic: 0.68145, Numeric: 0.22715\n",
      "END\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"END\")\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "# print(\"target_index is \\n\", target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"END\")\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss array \n",
      " [2.12692801 0.04858735]\n",
      "CHECK GRADIENT\n",
      "prediction is \n",
      " [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "loss array \n",
      " [2.12692801 0.04858735]\n",
      "analytic grad is \n",
      " [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "x shape[1]  2\n",
      "==========================================\n",
      "loss array \n",
      " [2.1269192  0.04858735]\n",
      "loss array \n",
      " [2.12693682 0.04858735]\n",
      "loss array \n",
      " [2.12693682 0.04858735]\n",
      "loss array \n",
      " [2.1269192  0.04858735]\n",
      "loss array \n",
      " [2.1269192  0.04858783]\n",
      "loss array \n",
      " [2.12693682 0.04858688]\n",
      "loss array \n",
      " [2.12693682 0.04858688]\n",
      "loss array \n",
      " [2.1269192  0.04858783]\n",
      "loss array \n",
      " [2.12693682 0.04858783]\n",
      "loss array \n",
      " [2.1269192  0.04858688]\n",
      "loss array \n",
      " [2.1269192  0.04858688]\n",
      "loss array \n",
      " [2.12693682 0.04858783]\n",
      "numeric grad array is \n",
      " [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "it is  -0.4403985389922482\n",
      "it is  0.4403985389922482\n",
      "it is  -0.4166856024112597\n",
      "it is  0.4166856024112597\n",
      "it is  0.4641114755732367\n",
      "it is  -0.4641114755732367\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is \n",
      " [[ 2  3  2  1]\n",
      " [ 3 42  4  1]\n",
      " [ 3  4  2  1]]\n",
      "c is \n",
      " [ 3 42  4]\n",
      "b is \n",
      " [[ -1   0  -1  -2]\n",
      " [-39   0 -38 -41]\n",
      " [ -1   0  -2  -3]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[[2,3,2,1], [3,42,4,1], [3,4,2,1]], [[2,3,2,1], [3,0,4,1], [3,4,2,1]], [[2,30,2,1], [3,1,4,1], [3,4,2,1]]])\n",
    "# print(a)\n",
    "# print(\"max is \\n\", np.max(a, axis=2))\n",
    "# print(a.ndim)\n",
    "\n",
    "\n",
    "b = np.array([[2,3,2,1], [3,42,4,1], [3,4,2,1]])\n",
    "c = np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "print (\"c is \\n\", c)\n",
    "for elem, i in zip(b, range(b.size)):\n",
    "    elem -= c[i]\n",
    "# print(\"f is \",f )\n",
    "# b -= np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "# probabilities = np.zeros_like(predictions)\n",
    "# sum_exps = np.sum(np.exp(b))\n",
    "# probabilities = np.exp(b)/sum_exps\n",
    "# print(\"probs are \", probabilities)\n",
    "\n",
    "dd = np.array([[3, 3]])\n",
    "print(len(dd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе. Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании вы:\n",
    "\n",
    "потренируетесь считать градиенты различных многомерных функций\n",
    "реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "реализуете процесс тренировки линейного классификатора\n",
    "подберете параметры тренировки на практике\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4728/3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/tmp/ipykernel_4728/3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic_grad  [6.]\n",
      "analytic_grad_at_ix  6.0\n",
      "it value is  3.0\n",
      "ix  (0,)\n",
      "zero arr is  [1.e-05]\n",
      "func1 is  8.999940000099999\n",
      "func2 is  9.0000600001\n",
      "numeric_grad_at_ix  6.000000000039306\n",
      "Gradient check passed!\n",
      "analytic_grad  [1. 1.]\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  3.0\n",
      "ix  (0,)\n",
      "zero arr is  [1.e-05 0.e+00]\n",
      "func1 is  4.99999\n",
      "func2 is  5.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  2.0\n",
      "ix  (1,)\n",
      "zero arr is  [0.e+00 1.e-05]\n",
      "func1 is  4.99999\n",
      "func2 is  5.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "Gradient check passed!\n",
      "analytic_grad  [[1. 1.]\n",
      " [1. 1.]]\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  3.0\n",
      "ix  (0, 0)\n",
      "zero arr is  [[1.e-05 0.e+00]\n",
      " [0.e+00 0.e+00]]\n",
      "func1 is  5.99999\n",
      "func2 is  6.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  2.0\n",
      "ix  (0, 1)\n",
      "zero arr is  [[0.e+00 1.e-05]\n",
      " [0.e+00 0.e+00]]\n",
      "func1 is  5.99999\n",
      "func2 is  6.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  1.0\n",
      "ix  (1, 0)\n",
      "zero arr is  [[0.e+00 0.e+00]\n",
      " [1.e-05 0.e+00]]\n",
      "func1 is  5.99999\n",
      "func2 is  6.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "analytic_grad_at_ix  1.0\n",
      "it value is  0.0\n",
      "ix  (1, 1)\n",
      "zero arr is  [[0.e+00 0.e+00]\n",
      " [0.e+00 1.e-05]]\n",
      "func1 is  5.99999\n",
      "func2 is  6.00001\n",
      "numeric_grad_at_ix  0.9999999999621422\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "90\n",
      "5\n",
      "[[ True  True False  True]\n",
      " [ True False False False]]\n",
      "(1, 0)\n",
      "new a is  [[-2 -3 -1  0]\n",
      " [-2 -2 -3 -4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3,2,4,5], [3,3,2,1]])\n",
    "b = np.array([3,2,90,5])\n",
    "for elem in b:\n",
    "    print(elem)\n",
    "\n",
    "check = np.isclose(a, b, atol=0.5)\n",
    "print(check)\n",
    "\n",
    "\n",
    "it = np.nditer(a, flags=['multi_index'], op_flags=['readwrite'])\n",
    "ix = it.multi_index\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "# it.()\n",
    "print(it.multi_index)\n",
    "\n",
    "a -= np.max(a)\n",
    "print(\"new a is \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions  [-10   0  10]\n",
      "probabilites are  [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "orig predictions  [1000    0    0]\n",
      "probabilites are  [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions  [-5  0  5]\n",
      "probabilites are  [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n",
      "loss is  5.006760443547122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions  [1 0 0]\n",
      "probabilites are  [0.57611688 0.21194156 0.21194156]\n",
      "loss is  1.551444713932051\n",
      "grad is  [ 0.57611688 -0.78805844  0.21194156]\n",
      "orig predictions  [1. 0. 0.]\n",
      "probabilites are  [0.57611688 0.21194156 0.21194156]\n",
      "loss is  1.551444713932051\n",
      "grad is  [ 0.57611688 -0.78805844  0.21194156]\n",
      "analytic_grad  [ 0.57611688 -0.78805844  0.21194156]\n",
      "analytic_grad_at_ix  0.5761168847658291\n",
      "it value is  1.0\n",
      "ix  (0,)\n",
      "zero arr is  [1.e-05 0.e+00 0.e+00]\n",
      "orig predictions  [1.00001 0.      0.     ]\n",
      "probabilites are  [0.57611933 0.21194034 0.21194034]\n",
      "loss is  1.551450475113109\n",
      "grad is  [ 0.57611933 -0.78805966  0.21194034]\n",
      "orig predictions  [0.99999 0.      0.     ]\n",
      "probabilites are  [0.57611444 0.21194278 0.21194278]\n",
      "loss is  1.5514389527754138\n",
      "grad is  [ 0.57611444 -0.78805722  0.21194278]\n",
      "func1 is  1.5514389527754138\n",
      "func2 is  1.551450475113109\n",
      "numeric_grad_at_ix  0.5761168847651099\n",
      "analytic_grad_at_ix  -0.7880584423829146\n",
      "it value is  0.0\n",
      "ix  (1,)\n",
      "zero arr is  [0.e+00 1.e-05 0.e+00]\n",
      "orig predictions  [1.e+00 1.e-05 0.e+00]\n",
      "probabilites are  [0.57611566 0.21194323 0.21194111]\n",
      "loss is  1.5514368333559785\n",
      "grad is  [ 0.57611566 -0.78805677  0.21194111]\n",
      "orig predictions  [ 1.e+00 -1.e-05  0.e+00]\n",
      "probabilites are  [0.57611811 0.21193989 0.21194201]\n",
      "loss is  1.5514525945248259\n",
      "grad is  [ 0.57611811 -0.78806011  0.21194201]\n",
      "func1 is  1.5514525945248259\n",
      "func2 is  1.5514368333559785\n",
      "numeric_grad_at_ix  -0.7880584423691771\n",
      "analytic_grad_at_ix  0.21194155761708544\n",
      "it value is  0.0\n",
      "ix  (2,)\n",
      "zero arr is  [0.e+00 0.e+00 1.e-05]\n",
      "orig predictions  [1.e+00 0.e+00 1.e-05]\n",
      "probabilites are  [0.57611566 0.21194111 0.21194323]\n",
      "loss is  1.5514468333559783\n",
      "grad is  [ 0.57611566 -0.78805889  0.21194323]\n",
      "orig predictions  [ 1.e+00  0.e+00 -1.e-05]\n",
      "probabilites are  [0.57611811 0.21194201 0.21193989]\n",
      "loss is  1.551442594524826\n",
      "grad is  [ 0.57611811 -0.78805799  0.21193989]\n",
      "func1 is  1.551442594524826\n",
      "func2 is  1.5514468333559783\n",
      "numeric_grad_at_ix  0.2119415576151695\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions  [[ 1.  2. -1.  1.]]\n",
      "max pred is  [2.]\n",
      "probabilites are  [[0.20603191 0.56005279 0.02788339 0.20603191]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4728/538432773.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Test batch_size = 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0morig_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalytic_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     assert np.all(np.isclose(orig_x, x, tol)\n\u001b[1;32m     37\u001b[0m                   ), \"Functions shouldn't modify input variables\"\n",
      "\u001b[0;32m/tmp/ipykernel_4728/538432773.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlinear_classifer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_with_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Test batch_size = 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/linear_classifer.py\u001b[0m in \u001b[0;36msoftmax_with_cross_entropy\u001b[0;34m(predictions, target_index)\u001b[0m\n\u001b[1;32m     78\u001b[0m     '''\n\u001b[1;32m     79\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;31m# print(\"probs are \", probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# print(\"loss is \", loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/linear_classifer.py\u001b[0m in \u001b[0;36mcross_entropy_loss\u001b[0;34m(probs, target_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# p_x = np.zeros_like(probs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# p_x[target_index] = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2  3  2  1]\n",
      "  [ 3 42  4  1]\n",
      "  [ 3  4  2  1]]\n",
      "\n",
      " [[ 2  3  2  1]\n",
      "  [ 3  0  4  1]\n",
      "  [ 3  4  2  1]]\n",
      "\n",
      " [[ 2 30  2  1]\n",
      "  [ 3  1  4  1]\n",
      "  [ 3  4  2  1]]]\n",
      "max is \n",
      " [[ 3 42  4]\n",
      " [ 3  4  4]\n",
      " [30  4  4]]\n",
      "3\n",
      "b is \n",
      " [[ 2  3  2  1]\n",
      " [ 3 42  4  1]\n",
      " [ 3  4  2  1]]\n",
      "c is \n",
      " [ 3 42  4]\n",
      "b is \n",
      " [[ -1   0  -1  -2]\n",
      " [-39   0 -38 -41]\n",
      " [ -1   0  -2  -3]]\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[[2,3,2,1], [3,42,4,1], [3,4,2,1]], [[2,3,2,1], [3,0,4,1], [3,4,2,1]], [[2,30,2,1], [3,1,4,1], [3,4,2,1]]])\n",
    "# print(a)\n",
    "# print(\"max is \\n\", np.max(a, axis=2))\n",
    "# print(a.ndim)\n",
    "\n",
    "\n",
    "b = np.array([[2,3,2,1], [3,42,4,1], [3,4,2,1]])\n",
    "c = np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "print (\"c is \\n\", c)\n",
    "for elem, i in zip(b, range(b.size)):\n",
    "    elem -= c[i]\n",
    "# print(\"f is \",f )\n",
    "# b -= np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "# probabilities = np.zeros_like(predictions)\n",
    "# sum_exps = np.sum(np.exp(b))\n",
    "# probabilities = np.exp(b)/sum_exps\n",
    "# print(\"probs are \", probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

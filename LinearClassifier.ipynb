{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе. Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании вы:\n",
    "\n",
    "потренируетесь считать градиенты различных многомерных функций\n",
    "реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "реализуете процесс тренировки линейного классификатора\n",
    "подберете параметры тренировки на практике\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12814/3741198026.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "/tmp/ipykernel_12814/3741198026.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "analytic grad is \n",
      " [6.]\n",
      "==========================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12814/3146107952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcheck_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0marray_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SimonSays/Mashine-Learning-Examples/gradient_check.py\u001b[0m in \u001b[0;36mcheck_gradient\u001b[0;34m(f, x, delta, tol)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==========================================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mzero_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mzero_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mfunc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mzero_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mfunc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mzero_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "90\n",
      "5\n",
      "[[ True  True False  True]\n",
      " [ True False False False]]\n",
      "(1, 0)\n",
      "new a is  [[-2 -3 -1  0]\n",
      " [-2 -2 -3 -4]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[3,2,4,5], [3,3,2,1]])\n",
    "b = np.array([3,2,90,5])\n",
    "for elem in b:\n",
    "    print(elem)\n",
    "\n",
    "check = np.isclose(a, b, atol=0.5)\n",
    "print(check)\n",
    "\n",
    "\n",
    "it = np.nditer(a, flags=['multi_index'], op_flags=['readwrite'])\n",
    "ix = it.multi_index\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "it.iternext()\n",
    "# it.()\n",
    "print(it.multi_index)\n",
    "\n",
    "a -= np.max(a)\n",
    "print(\"new a is \", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions \n",
      " [-10   0  10]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [2.06106005e-09 4.53978686e-05 9.99954600e-01]\n",
      "orig predictions \n",
      " [1000    0    0]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions \n",
      " [-5  0  5]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [4.50940412e-05 6.69254912e-03 9.93262357e-01]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[5.00676044]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.00676044]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig predictions \n",
      " [1 0 0]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611688 0.21194156 0.21194156]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55144471]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611688 -0.78805844  0.21194156]\n",
      "CHECK GRADIENT\n",
      "orig predictions \n",
      " [1. 0. 0.]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611688 0.21194156 0.21194156]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55144471]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611688 -0.78805844  0.21194156]\n",
      "analytic grad is \n",
      " [ 0.57611688 -0.78805844  0.21194156]\n",
      "x is \n",
      " [1. 0. 0.]\n",
      "orig predictions \n",
      " [1.00001 0.      0.     ]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611933 0.21194034 0.21194034]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55145048]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611933 -0.78805966  0.21194034]\n",
      "orig predictions \n",
      " [0.99999 0.      0.     ]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611444 0.21194278 0.21194278]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55143895]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611444 -0.78805722  0.21194278]\n",
      "orig predictions \n",
      " [1.e+00 1.e-05 0.e+00]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611566 0.21194323 0.21194111]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55143683]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611566 -0.78805677  0.21194111]\n",
      "orig predictions \n",
      " [ 1.e+00 -1.e-05  0.e+00]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611811 0.21193989 0.21194201]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55145259]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611811 -0.78806011  0.21194201]\n",
      "orig predictions \n",
      " [1.e+00 0.e+00 1.e-05]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611566 0.21194111 0.21194323]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55144683]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611566 -0.78805889  0.21194323]\n",
      "orig predictions \n",
      " [ 1.e+00  0.e+00 -1.e-05]\n",
      "batch_size is  3\n",
      "dimension is  1\n",
      "probabilites are \n",
      " [0.57611811 0.21194201 0.21193989]\n",
      "targ ind is  [[1]]\n",
      "1\n",
      "probs  (3,)\n",
      "loss is  [[1.55144259]]\n",
      "target index\n",
      " [[1]]\n",
      "1\n",
      "grad is \n",
      " [ 0.57611811 -0.78805799  0.21193989]\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([[1]]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([[1]])), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "loss array \n",
      " [3.57972422 3.38352864 1.2142833 ]\n",
      "loss is  2.7258453874455473\n",
      "analytic grad is \n",
      " [[ 0.20603191  0.56005279 -0.97211661  0.20603191]\n",
      " [ 0.25069239  0.68145256 -0.96607247  0.03392753]\n",
      " [ 0.29692274  0.10923177  0.29692274 -0.70307726]]\n",
      "analytic average grad is \n",
      " [ 0.25121568  0.45024571 -0.54708878 -0.15437261]\n",
      "prediction is \n",
      " [[ 1.  2. -1.  1.]\n",
      " [ 1.  2. -1. -1.]\n",
      " [ 1.  0.  1.  1.]]\n",
      "==========================================\n",
      "loss array \n",
      " [3.57972628 3.38353115 1.21428627]\n",
      "loss is  2.7258478996116815\n",
      "loss array \n",
      " [3.57972216 3.38352613 1.21428033]\n",
      "loss is  2.725842875298086\n",
      "loss array \n",
      " [3.57972982 3.38353545 1.21428439]\n",
      "loss is  2.725849889911991\n",
      "loss array \n",
      " [3.57971862 3.38352182 1.21428221]\n",
      "loss is  2.725840884997796\n",
      "loss array \n",
      " [3.5797145  3.38351898 1.21428627]\n",
      "loss is  2.7258399165622067\n",
      "loss array \n",
      " [3.57973394 3.3835383  1.21428033]\n",
      "loss is  2.725850858337843\n",
      "loss array \n",
      " [3.57972628 3.38352898 1.21427627]\n",
      "loss is  2.7258438437262225\n",
      "loss array \n",
      " [3.57972216 3.3835283  1.21429033]\n",
      "loss is  2.725846931178376\n",
      "numeric grad array is \n",
      " [ 0.25121568  0.45024571 -0.54708878 -0.15437261]\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "# num_classes = 4\n",
    "# batch_size = 1\n",
    "# predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "# target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "# check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "# print(\"END\")\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "# print(\"target_index is \\n\", target_index)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"END\")\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "# probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "# assert np.all(np.isclose(probs[:, 0], 1.0))\n",
    "# print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b is \n",
      " [[ 2  3  2  1]\n",
      " [ 3 42  4  1]\n",
      " [ 3  4  2  1]]\n",
      "c is \n",
      " [ 3 42  4]\n",
      "b is \n",
      " [[ -1   0  -1  -2]\n",
      " [-39   0 -38 -41]\n",
      " [ -1   0  -2  -3]]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[[2,3,2,1], [3,42,4,1], [3,4,2,1]], [[2,3,2,1], [3,0,4,1], [3,4,2,1]], [[2,30,2,1], [3,1,4,1], [3,4,2,1]]])\n",
    "# print(a)\n",
    "# print(\"max is \\n\", np.max(a, axis=2))\n",
    "# print(a.ndim)\n",
    "\n",
    "\n",
    "b = np.array([[2,3,2,1], [3,42,4,1], [3,4,2,1]])\n",
    "c = np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "print (\"c is \\n\", c)\n",
    "for elem, i in zip(b, range(b.size)):\n",
    "    elem -= c[i]\n",
    "# print(\"f is \",f )\n",
    "# b -= np.max(b, axis=1)\n",
    "print(\"b is \\n\",b)\n",
    "# probabilities = np.zeros_like(predictions)\n",
    "# sum_exps = np.sum(np.exp(b))\n",
    "# probabilities = np.exp(b)/sum_exps\n",
    "# print(\"probs are \", probabilities)\n",
    "\n",
    "dd = np.array([[3, 3]])\n",
    "print(len(dd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

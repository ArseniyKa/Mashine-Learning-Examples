{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе. Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании вы:\n",
    "\n",
    "потренируетесь считать градиенты различных многомерных функций\n",
    "реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "реализуете процесс тренировки линейного классификатора\n",
    "подберете параметры тренировки на практике\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old shape  (10000, 32, 32, 3)\n",
      "train_X shape  (9000, 3073)\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "old_shape = train_X.shape\n",
    "print(\"old shape \", old_shape)\n",
    "old_test_X = test_X.copy()\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)\n",
    "print(\"train_X shape \", train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [3.]\n",
      "analytic grad is \n",
      " [6.]\n",
      "numeric grad array is \n",
      " [6.]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [3. 2.]\n",
      "analytic grad is \n",
      " [1. 1.]\n",
      "numeric grad array is \n",
      " [1. 1.]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [[3. 2.]\n",
      " [1. 0.]]\n",
      "analytic grad is \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "numeric grad array is \n",
      " [[1. 1.]\n",
      " [1. 1.]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss array \n",
      " [5.00676044]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, np.array([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss array \n",
      " [1.55144471]\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [1. 0. 0.]\n",
      "loss array \n",
      " [1.55144471]\n",
      "analytic grad is \n",
      " [ 0.57611688 -0.78805844  0.21194156]\n",
      "loss array \n",
      " [1.55145048]\n",
      "loss array \n",
      " [1.55143895]\n",
      "loss array \n",
      " [1.55143683]\n",
      "loss array \n",
      " [1.55145259]\n",
      "loss array \n",
      " [1.55144683]\n",
      "loss array \n",
      " [1.55144259]\n",
      "numeric grad array is \n",
      " [ 0.57611688 -0.78805844  0.21194156]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), np.array([[1]]))\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, np.array([[1]])), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [[ 1.  2. -1.  1.]]\n",
      "loss array \n",
      " [3.57972422]\n",
      "analytic grad is \n",
      " [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "loss array \n",
      " [3.57972628]\n",
      "loss array \n",
      " [3.57972216]\n",
      "loss array \n",
      " [3.57972982]\n",
      "loss array \n",
      " [3.57971862]\n",
      "loss array \n",
      " [3.5797145]\n",
      "loss array \n",
      " [3.57973394]\n",
      "loss array \n",
      " [3.57972628]\n",
      "loss array \n",
      " [3.57972216]\n",
      "numeric grad array is \n",
      " [[ 0.20603191  0.56005279 -0.97211661  0.20603191]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "END\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [ 2. -1. -1.  1.]\n",
      "loss array \n",
      " [1.38352864]\n",
      "analytic grad is \n",
      " [ 0.68145256  0.03392753  0.03392753 -0.74930761]\n",
      "loss array \n",
      " [1.38353545]\n",
      "loss array \n",
      " [1.38352182]\n",
      "loss array \n",
      " [1.38352898]\n",
      "loss array \n",
      " [1.3835283]\n",
      "loss array \n",
      " [1.38352898]\n",
      "loss array \n",
      " [1.3835283]\n",
      "loss array \n",
      " [1.38352115]\n",
      "loss array \n",
      " [1.38353613]\n",
      "numeric grad array is \n",
      " [ 0.68145256  0.03392753  0.03392753 -0.74930761]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [0. 1. 1. 1.]\n",
      "loss array \n",
      " [1.2142833]\n",
      "analytic grad is \n",
      " [ 0.10923177  0.29692274  0.29692274 -0.70307726]\n",
      "loss array \n",
      " [1.21428439]\n",
      "loss array \n",
      " [1.21428221]\n",
      "loss array \n",
      " [1.21428627]\n",
      "loss array \n",
      " [1.21428033]\n",
      "loss array \n",
      " [1.21428627]\n",
      "loss array \n",
      " [1.21428033]\n",
      "loss array \n",
      " [1.21427627]\n",
      "loss array \n",
      " [1.21429033]\n",
      "numeric grad array is \n",
      " [ 0.10923177  0.29692274  0.29692274 -0.70307726]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [ 1.  2. -1.  2.]\n",
      "loss array \n",
      " [3.88280282]\n",
      "analytic grad is \n",
      " [ 0.15216302  0.41362198 -0.97940697  0.41362198]\n",
      "loss array \n",
      " [3.88280434]\n",
      "loss array \n",
      " [3.8828013]\n",
      "loss array \n",
      " [3.88280696]\n",
      "loss array \n",
      " [3.88279869]\n",
      "loss array \n",
      " [3.88279303]\n",
      "loss array \n",
      " [3.88281262]\n",
      "loss array \n",
      " [3.88280696]\n",
      "loss array \n",
      " [3.88279869]\n",
      "numeric grad array is \n",
      " [ 0.15216302  0.41362198 -0.97940697  0.41362198]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "END\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"END\")\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "# print(\"target_index is \\n\", target_index)\n",
    "for ind in range(target_index.size):\n",
    "    check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index[ind]), predictions[ind])\n",
    "print(\"END\")\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss array \n",
      " [2.12692801 0.04858735]\n",
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "loss array \n",
      " [2.12692801 0.04858735]\n",
      "analytic grad is \n",
      " [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "loss array \n",
      " [2.1269192  0.04858735]\n",
      "loss array \n",
      " [2.12693682 0.04858735]\n",
      "loss array \n",
      " [2.12693682 0.04858735]\n",
      "loss array \n",
      " [2.1269192  0.04858735]\n",
      "loss array \n",
      " [2.1269192  0.04858783]\n",
      "loss array \n",
      " [2.12693682 0.04858688]\n",
      "loss array \n",
      " [2.12693682 0.04858688]\n",
      "loss array \n",
      " [2.1269192  0.04858783]\n",
      "loss array \n",
      " [2.12693682 0.04858783]\n",
      "loss array \n",
      " [2.1269192  0.04858688]\n",
      "loss array \n",
      " [2.1269192  0.04858688]\n",
      "loss array \n",
      " [2.12693682 0.04858783]\n",
      "numeric grad array is \n",
      " [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "predictions is \n",
      " [[ 1.  2.]\n",
      " [-1.  1.]\n",
      " [ 1.  2.]]\n",
      "analytic grad is \n",
      " [[ 0.02  0.04]\n",
      " [-0.02  0.02]\n",
      " [ 0.02  0.04]]\n",
      "numeric grad array is \n",
      " [[ 0.02  0.04]\n",
      " [-0.02  0.02]\n",
      " [ 0.02  0.04]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Тренировка!**\n",
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape  (9000, 3073)\n",
      "num train is  9000\n",
      "num features is  3073\n",
      "num classes  is  10\n",
      "Epoch 0, loss: 2.302733\n",
      "Epoch 0, loss: 2.302654\n",
      "Epoch 0, loss: 2.302909\n",
      "Epoch 0, loss: 2.303238\n",
      "Epoch 0, loss: 2.302780\n",
      "Epoch 0, loss: 2.303146\n",
      "Epoch 0, loss: 2.302941\n",
      "Epoch 0, loss: 2.303224\n",
      "Epoch 0, loss: 2.303448\n",
      "Epoch 0, loss: 2.302550\n",
      "Epoch 0, loss: 2.302793\n",
      "Epoch 0, loss: 2.302149\n",
      "Epoch 0, loss: 2.302653\n",
      "Epoch 0, loss: 2.302907\n",
      "Epoch 0, loss: 2.303601\n",
      "Epoch 0, loss: 2.303475\n",
      "Epoch 0, loss: 2.303150\n",
      "Epoch 0, loss: 2.302799\n",
      "Epoch 0, loss: 2.303245\n",
      "Epoch 0, loss: 2.303216\n",
      "Epoch 0, loss: 2.302992\n",
      "Epoch 0, loss: 2.302874\n",
      "Epoch 0, loss: 2.302313\n",
      "Epoch 0, loss: 2.303245\n",
      "Epoch 0, loss: 2.302283\n",
      "Epoch 0, loss: 2.302938\n",
      "Epoch 0, loss: 2.302587\n",
      "Epoch 0, loss: 2.303513\n",
      "Epoch 0, loss: 2.301987\n",
      "Epoch 0, loss: 2.302917\n",
      "Linear softmax classifier test set accuracy: 8.922222\n",
      "Epoch 1, loss: 2.302426\n",
      "Epoch 1, loss: 2.303128\n",
      "Epoch 1, loss: 2.302140\n",
      "Epoch 1, loss: 2.302245\n",
      "Epoch 1, loss: 2.303473\n",
      "Epoch 1, loss: 2.302723\n",
      "Epoch 1, loss: 2.302970\n",
      "Epoch 1, loss: 2.302861\n",
      "Epoch 1, loss: 2.302756\n",
      "Epoch 1, loss: 2.302173\n",
      "Epoch 1, loss: 2.303069\n",
      "Epoch 1, loss: 2.302716\n",
      "Epoch 1, loss: 2.302844\n",
      "Epoch 1, loss: 2.301515\n",
      "Epoch 1, loss: 2.302768\n",
      "Epoch 1, loss: 2.302600\n",
      "Epoch 1, loss: 2.303042\n",
      "Epoch 1, loss: 2.302026\n",
      "Epoch 1, loss: 2.303031\n",
      "Epoch 1, loss: 2.302402\n",
      "Epoch 1, loss: 2.302119\n",
      "Epoch 1, loss: 2.303150\n",
      "Epoch 1, loss: 2.303135\n",
      "Epoch 1, loss: 2.303417\n",
      "Epoch 1, loss: 2.303405\n",
      "Epoch 1, loss: 2.302845\n",
      "Epoch 1, loss: 2.303353\n",
      "Epoch 1, loss: 2.302909\n",
      "Epoch 1, loss: 2.303336\n",
      "Epoch 1, loss: 2.303291\n",
      "Linear softmax classifier test set accuracy: 9.066667\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "print(\"train X shape \", train_X.shape)\n",
    "ind = np.array([1,5,222])\n",
    "# print(train_X[ind])\n",
    "\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=2, learning_rate=0.0001, batch_size=300, reg=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7facbbd85ac0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test pred is \n",
      " [9 9 9 8 9 9 8 3 9 9]\n",
      "test_y  is \n",
      " [2 1 3 0 2 5 0 1 7 1]\n",
      "Linear softmax classifier test set accuracy: 9.600000\n"
     ]
    }
   ],
   "source": [
    "# classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "# print(\"features is \", test_X.shape[1])\n",
    "# classifier.initW(test_X.shape[0],10)\n",
    "\n",
    "\n",
    "# print(\"test x is \", test_X.shape)\n",
    "# test_pred = classifier.predict(test_X[:10])\n",
    "# print(\"test pred is \", test_pred)\n",
    "# label = np.argmax(test_pred, axis=1)\n",
    "# # print(\"test pred \", test_pred)\n",
    "# print(\"label is \", label)\n",
    "# # plt.imshow(old_test_X[3].astype(np.uint8))\n",
    "# number = 10\n",
    "# plot_index = 1\n",
    "# for class_index in range(number):\n",
    "#         plt.subplot(1, number, plot_index)\n",
    "        \n",
    "#         image = old_test_X[class_index]\n",
    "#         plt.imshow(image.astype(np.uint8))\n",
    "#         plt.axis('off')\n",
    "#         plot_index += 1\n",
    "\n",
    "# plt.show()\n",
    "test_pred = classifier.predict(test_X)\n",
    "test_pred = np.argmax(test_pred, axis=1)\n",
    "print(\"test pred is \\n\", test_pred[:10])\n",
    "print(\"test_y  is \\n\", test_y[:10])\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)*100\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range is \n",
      " range(0, 3)\n",
      "indices is \n",
      " [1 1 2]\n",
      "b is \n",
      " [[ 2  3  2  1]\n",
      " [ 3 42  4  1]\n",
      " [ 3  4  2  1]]\n",
      "definite b is \n",
      " [ 3 42  2]\n",
      "part  is \n",
      " [-1.  1. -1.]\n",
      "arr is \n",
      " [[ 2. -1. -1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 1.  2. -1.  2.]]\n",
      "part  is \n",
      " [-1.  1. -1.]\n",
      "new arr is \n",
      " [[ 2. -2. -1.  1.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 1.  2. -2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# a = np.array([[[2,3,2,1], [3,42,4,1], [3,4,2,1]], [[2,3,2,1], [3,0,4,1], [3,4,2,1]], [[2,30,2,1], [3,1,4,1], [3,4,2,1]]])\n",
    "# print(a)\n",
    "# print(\"max is \\n\", np.max(a, axis=2))\n",
    "# print(a.ndim)\n",
    "\n",
    "\n",
    "b = np.array([[2,3,2,1], [3,42,4,1], [3,4,2,1]])\n",
    "indices = np.array([1,1,2])\n",
    "print(\"range is \\n\",range(len(b)))\n",
    "output_array = b[range(len(b)) , indices ]\n",
    "# indices = indices[:, np.newaxis]\n",
    "print(\"indices is \\n\", indices)\n",
    "print(\"b is \\n\",b)\n",
    "print(\"definite b is \\n\", output_array)\n",
    "# b_max = np.max(b, axis=1)\n",
    "# b_max = b_max[:, np.newaxis]\n",
    "# print(\"b_max is \\n\", b_max)\n",
    "# # print(\"b is \\n\",b)\n",
    "# c = b - b_max\n",
    "# # c = np.subtract(b, c)\n",
    "# print(\"c is \\n\",c)\n",
    "\n",
    "# sum_exps = np.sum(np.exp(b),  axis=1)\n",
    "# sum_exps = sum_exps[:, np.newaxis]\n",
    "# print(\"sum exps\\n\",sum_exps)\n",
    "# probabilities = np.exp(b)/sum_exps\n",
    "# print(\"probabilities\\n\", probabilities)\n",
    "arr = np.array([[ 2., -1., -1. , 1.], [ 0.,  1. , 1. , 1.] ,[ 1.,  2. ,-1. , 2.]])\n",
    "part = arr[range(len(arr)), indices]\n",
    "print(\"part  is \\n\", part)\n",
    "print(\"arr is \\n\", arr)\n",
    "arr[range(len(arr)), indices]-=1\n",
    "print(\"part  is \\n\", part)\n",
    "print(\"new arr is \\n\", arr)\n",
    "\n",
    "# print(b[range(len(b)), indices])\n",
    "# a = np.array([[1,2,3], [4,5,6], [7,8,9]])\n",
    "# a = a[np.arange(len(a)), [1,0,2]]\n",
    "# print(\"a is \\n\", a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8d02a8b3f9d4449c292051e97c4fa6280aba47e831569dc744e632849a185d52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.2 - Введение в PyTorch\n",
    "\n",
    "Для этого задания потребуется установить версию PyTorch 1.0\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "В этом задании мы познакомимся с основными компонентами PyTorch и натренируем несколько небольших моделей.\n",
    "GPU нам пока не понадобится.\n",
    "\n",
    "Основные ссылки:\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "https://pytorch.org/docs/stable/torchvision/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets load the dataset\n",
    "data_train = dset.SVHN('./data/', split='train',\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])                           \n",
    "                       ])\n",
    "                      )\n",
    "data_test = dset.SVHN('./data/', split='test', \n",
    "                      transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])                           \n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы разделим данные на training и validation с использованием классов SubsetRandomSampler и DataLoader.\n",
    "\n",
    "DataLoader подгружает данные, предоставляемые классом Dataset, во время тренировки и группирует их в батчи. Он дает возможность указать Sampler, который выбирает, какие примеры из датасета использовать для тренировки. Мы используем это, чтобы разделить данные на training и validation.\n",
    "\n",
    "Подробнее: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices is\n",
      " 14651\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "data_size = data_train.data.shape[0]\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "print(\"indices is\\n\", len(val_sampler))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей задаче мы получаем на вход изображения, но работаем с ними как с одномерными массивами. Чтобы превратить многомерный массив в одномерный, мы воспользуемся очень простым вспомогательным модулем Flattener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVHN data sample shape:  torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "sample, label = data_train[0]\n",
    "print(\"SVHN data sample shape: \", sample.shape)\n",
    "# As you can see, the data is shaped like an image\n",
    "\n",
    "# We'll use a special helper module to shape it into a tensor\n",
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец, мы создаем основные объекты PyTorch:\n",
    "\n",
    "nn_model - собственно, модель с нейросетью\n",
    "\n",
    "loss - функцию ошибки, в нашем случае CrossEntropyLoss\n",
    "\n",
    "optimizer - алгоритм оптимизации, в нашем случае просто SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(3*32*32, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "# We will minimize cross-entropy between the ground truth and\n",
    "# network predictions using an SGD optimizer\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренируем!\n",
    "\n",
    "Ниже приведена функция train_model, реализующая основной цикл тренировки PyTorch.\n",
    "\n",
    "Каждую эпоху эта функция вызывает функцию compute_accuracy, которая вычисляет точность на validation, эту последнюю функцию предлагается реализовать вам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.834957, Train accuracy: 0.407655, Val accuracy: 0.555866\n",
      "Average loss: 1.460087, Train accuracy: 0.583831, Val accuracy: 0.602484\n",
      "Average loss: 1.380018, Train accuracy: 0.619169, Val accuracy: 0.623848\n"
     ]
    }
   ],
   "source": [
    "# This is how to implement the same main train loop in PyTorch. Pretty easy, right?\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler=None):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "\n",
    "        if not scheduler  is None:\n",
    "            scheduler.step()\n",
    "            print(\"step\")\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    correct_samples = 0\n",
    "    total_samples = 0         \n",
    "    for i_step, (x, y) in enumerate(val_loader):\n",
    "        count = i_step\n",
    "        predictions = nn_model(x)\n",
    "        pred = torch.argmax(predictions, axis=1)\n",
    "        correct_samples += torch.sum(pred == y)\n",
    "        total_samples += y.shape[0]\n",
    "\n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "    # print(\"val accuracy is \", val_accuracy)       \n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "    \n",
    "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
    "    #       and compute the overall accuracy.\n",
    "    # Hint: PyTorch has the argmax function!\n",
    "    \n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# val_loader.\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# После основного цикла\n",
    "\n",
    "Посмотрим на другие возможности и оптимизации, которые предоставляет PyTorch.\n",
    "\n",
    "Добавьте еще один скрытый слой размера 100 нейронов к модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 2.169266, Train accuracy: 0.215183, Val accuracy: 0.252747\n",
      "Average loss: 2.007896, Train accuracy: 0.275774, Val accuracy: 0.281073\n",
      "Average loss: 1.854946, Train accuracy: 0.334130, Val accuracy: 0.370623\n",
      "Average loss: 1.731816, Train accuracy: 0.406887, Val accuracy: 0.407617\n",
      "Average loss: 1.682593, Train accuracy: 0.431731, Val accuracy: 0.423521\n"
     ]
    }
   ],
   "source": [
    "# Since it's so easy to add layers, let's add some!\n",
    "\n",
    "# TODO: Implement a model with 2 hidden layers of the size 100\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(3*32*32, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 2.063724, Train accuracy: 0.314609, Val accuracy: 0.421063\n",
      "Average loss: 1.754835, Train accuracy: 0.463639, Val accuracy: 0.509522\n",
      "Average loss: 1.619905, Train accuracy: 0.534297, Val accuracy: 0.566446\n",
      "Average loss: 1.534069, Train accuracy: 0.575299, Val accuracy: 0.591086\n",
      "Average loss: 1.473195, Train accuracy: 0.601679, Val accuracy: 0.613200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We heard batch normalization is powerful, let's use it!\n",
    "# TODO: Add batch normalization after each of the hidden layers of the network, before or after non-linearity\n",
    "# Hint: check out torch.nn.BatchNorm1d\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(3*32*32, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step\n",
      "Average loss: 2.074605, Train accuracy: 0.305993, Val accuracy: 0.445908\n",
      "step\n",
      "Average loss: 1.759303, Train accuracy: 0.484677, Val accuracy: 0.533138\n",
      "step\n",
      "Average loss: 1.599192, Train accuracy: 0.566085, Val accuracy: 0.597024\n",
      "step\n",
      "Average loss: 1.481816, Train accuracy: 0.609579, Val accuracy: 0.632244\n",
      "step\n",
      "Average loss: 1.469575, Train accuracy: 0.609494, Val accuracy: 0.649375\n"
     ]
    }
   ],
   "source": [
    "# Learning rate annealing\n",
    "# Reduce your learning rate 2x every 2 epochs\n",
    "# Hint: look up learning rate schedulers in PyTorch. You might need to extend train_model function a little bit too!\n",
    "\n",
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(3*32*32, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
    "scheduler =  torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma= 2)\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализируем ошибки модели\n",
    "\n",
    "Попробуем посмотреть, на каких изображениях наша модель ошибается. Для этого мы получим все предсказания модели на validation set и сравним их с истинными метками (ground truth).\n",
    "\n",
    "Первая часть - реализовать код на PyTorch, который вычисляет все предсказания модели на validation set.\n",
    "Чтобы это сделать мы приводим код SubsetSampler, который просто проходит по всем заданным индексам последовательно и составляет из них батчи.\n",
    "\n",
    "Реализуйте функцию evaluate_model, которая прогоняет модель через все сэмплы validation set и запоминает предсказания модели и истинные метки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len pred  14651\n",
      "len indices  14651\n",
      "pred type is  <class 'numpy.ndarray'>\n",
      "pred  is  [2]\n",
      "gt  is  [3]\n"
     ]
    }
   ],
   "source": [
    "class SubsetSampler(Sampler):\n",
    "    r\"\"\"Samples elements with given indices sequentially\n",
    "\n",
    "    Arguments:\n",
    "        indices (ndarray): indices of the samples to take\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    \n",
    "def evaluate_model(model, dataset, indices):\n",
    "    \"\"\"\n",
    "    Computes predictions and ground truth labels for the indices of the dataset\n",
    "    \n",
    "    Returns: \n",
    "    predictions: np array of ints - model predictions\n",
    "    grount_truth: np array of ints - actual labels of the dataset\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    val_sampler = SubsetSampler(indices)\n",
    "    val_loader = torch.utils.data.DataLoader(data_train, batch_size=1,\n",
    "                                         sampler=val_sampler)\n",
    "\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for i_step, (x, y) in enumerate(val_loader):\n",
    "            pred = model(x)\n",
    "            pred =  torch.argmax(pred, axis=1)\n",
    "            predictions.append(pred.detach().numpy())\n",
    "            ground_truth.append(y.detach().numpy())\n",
    "\n",
    "    # predictions = np.array(predictions)\n",
    "    # ground_truth = np.array(ground_truth)\n",
    "    # TODO: Evaluate model on the list of indices and capture predictions\n",
    "    # and ground truth labels\n",
    "    # Hint: SubsetSampler above could be useful!\n",
    "    \n",
    "    # raise Exception(\"Not implemented\")\n",
    "    print('len pred ', len(predictions))\n",
    "    print('len indices ', len(indices))\n",
    "    return predictions, ground_truth\n",
    "\n",
    "# Evaluate model on validation\n",
    "predictions, gt = evaluate_model(nn_model, data_train, val_indices)\n",
    "assert len(predictions) == len(val_indices)\n",
    "assert len(gt) == len(val_indices)\n",
    "print(\"pred type is \", type(predictions[0]))\n",
    "print(\"pred  is \", predictions[0])\n",
    "print(\"gt  is \", gt[0])\n",
    "\n",
    "assert gt[100] == data_train[val_indices[100]][1]\n",
    "assert np.any(np.not_equal(gt, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6326530612244898\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "            # _, indices = torch.max(prediction, 1)\n",
    "            # correct_samples += torch.sum(indices == y)\n",
    "            # total_samples += y.shape[0]\n",
    "\n",
    "correct_samples = 0\n",
    "total_samples = 0         \n",
    "for i_step, (x, y) in enumerate(val_loader):\n",
    "    count = i_step\n",
    "    predictions = nn_model(x)\n",
    "    pred = torch.argmax(predictions, axis=1)\n",
    "    correct_samples += torch.sum(pred == y)\n",
    "    total_samples += y.shape[0]\n",
    "    # print(y.shape)\n",
    "    # print(i_step)\n",
    "\n",
    "val_accuracy = float(correct_samples) / total_samples\n",
    "print(val_accuracy)\n",
    "# print(count)\n",
    "# val_loader.dataset.data\n",
    "# print(y.shape)\n",
    "# print(data_train.data.shape)\n",
    "\n",
    "# all_y = val_loader.dataset.labels\n",
    "# all_X = val_loader.dataset.data\n",
    "# # nn_model.eval() # Evaluation mode\n",
    "# predictions = nn_model(all_X[0])\n",
    "# print(\"pred shape \", all_X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

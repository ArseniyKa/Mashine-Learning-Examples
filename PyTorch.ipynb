{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.2 - Введение в PyTorch\n",
    "\n",
    "Для этого задания потребуется установить версию PyTorch 1.0\n",
    "\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "В этом задании мы познакомимся с основными компонентами PyTorch и натренируем несколько небольших моделей.\n",
    "GPU нам пока не понадобится.\n",
    "\n",
    "Основные ссылки:\n",
    "https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "https://pytorch.org/docs/stable/nn.html\n",
    "https://pytorch.org/docs/stable/torchvision/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets load the dataset\n",
    "data_train = dset.SVHN('./data/', split='train',\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])                           \n",
    "                       ])\n",
    "                      )\n",
    "data_test = dset.SVHN('./data/', split='test', \n",
    "                      transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.43,0.44,0.47],\n",
    "                                               std=[0.20,0.20,0.20])                           \n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы разделим данные на training и validation с использованием классов SubsetRandomSampler и DataLoader.\n",
    "\n",
    "DataLoader подгружает данные, предоставляемые классом Dataset, во время тренировки и группирует их в батчи. Он дает возможность указать Sampler, который выбирает, какие примеры из датасета использовать для тренировки. Мы используем это, чтобы разделить данные на training и validation.\n",
    "\n",
    "Подробнее: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "data_size = data_train.data.shape[0]\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей задаче мы получаем на вход изображения, но работаем с ними как с одномерными массивами. Чтобы превратить многомерный массив в одномерный, мы воспользуемся очень простым вспомогательным модулем Flattener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVHN data sample shape:  torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "sample, label = data_train[0]\n",
    "print(\"SVHN data sample shape: \", sample.shape)\n",
    "# As you can see, the data is shaped like an image\n",
    "\n",
    "# We'll use a special helper module to shape it into a tensor\n",
    "class Flattener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size, *_ = x.shape\n",
    "        return x.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец, мы создаем основные объекты PyTorch:\n",
    "\n",
    "nn_model - собственно, модель с нейросетью\n",
    "\n",
    "loss - функцию ошибки, в нашем случае CrossEntropyLoss\n",
    "\n",
    "optimizer - алгоритм оптимизации, в нашем случае просто SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = nn.Sequential(\n",
    "            Flattener(),\n",
    "            nn.Linear(3*32*32, 100),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 10), \n",
    "         )\n",
    "nn_model.type(torch.FloatTensor)\n",
    "\n",
    "# We will minimize cross-entropy between the ground truth and\n",
    "# network predictions using an SGD optimizer\n",
    "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
    "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренируем!\n",
    "\n",
    "Ниже приведена функция train_model, реализующая основной цикл тренировки PyTorch.\n",
    "\n",
    "Каждую эпоху эта функция вызывает функцию compute_accuracy, которая вычисляет точность на validation, эту последнюю функцию предлагается реализовать вам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss: 1.307358, Train accuracy: 0.657168, Val accuracy: 0.646236\n",
      "Average loss: 1.306368, Train accuracy: 0.657134, Val accuracy: 0.643642\n",
      "Average loss: 1.306151, Train accuracy: 0.658960, Val accuracy: 0.642072\n"
     ]
    }
   ],
   "source": [
    "# This is how to implement the same main train loop in PyTorch. Pretty easy, right?\n",
    "\n",
    "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs):    \n",
    "    loss_history = []\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "        \n",
    "        loss_accum = 0\n",
    "        correct_samples = 0\n",
    "        total_samples = 0\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            prediction = model(x)    \n",
    "            loss_value = loss(prediction, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            _, indices = torch.max(prediction, 1)\n",
    "            correct_samples += torch.sum(indices == y)\n",
    "            total_samples += y.shape[0]\n",
    "            \n",
    "            loss_accum += loss_value\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "        train_accuracy = float(correct_samples) / total_samples\n",
    "        val_accuracy = compute_accuracy(model, val_loader)\n",
    "        \n",
    "        loss_history.append(float(ave_loss))\n",
    "        train_history.append(train_accuracy)\n",
    "        val_history.append(val_accuracy)\n",
    "        \n",
    "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
    "        \n",
    "    return loss_history, train_history, val_history\n",
    "        \n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "    \n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "    correct_samples = 0\n",
    "    total_samples = 0         \n",
    "    for i_step, (x, y) in enumerate(val_loader):\n",
    "        count = i_step\n",
    "        predictions = nn_model(x)\n",
    "        pred = torch.argmax(predictions, axis=1)\n",
    "        correct_samples += torch.sum(pred == y)\n",
    "        total_samples += y.shape[0]\n",
    "\n",
    "    val_accuracy = float(correct_samples) / total_samples\n",
    "    # print(\"val accuracy is \", val_accuracy)\n",
    "        \n",
    "\n",
    "    return val_accuracy\n",
    "\n",
    "\n",
    "    \n",
    "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
    "    #       and compute the overall accuracy.\n",
    "    # Hint: PyTorch has the argmax function!\n",
    "    \n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# val_loader.\n",
    "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6326530612244898\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "            # _, indices = torch.max(prediction, 1)\n",
    "            # correct_samples += torch.sum(indices == y)\n",
    "            # total_samples += y.shape[0]\n",
    "\n",
    "correct_samples = 0\n",
    "total_samples = 0         \n",
    "for i_step, (x, y) in enumerate(val_loader):\n",
    "    count = i_step\n",
    "    predictions = nn_model(x)\n",
    "    pred = torch.argmax(predictions, axis=1)\n",
    "    correct_samples += torch.sum(pred == y)\n",
    "    total_samples += y.shape[0]\n",
    "    # print(y.shape)\n",
    "    # print(i_step)\n",
    "\n",
    "val_accuracy = float(correct_samples) / total_samples\n",
    "print(val_accuracy)\n",
    "# print(count)\n",
    "# val_loader.dataset.data\n",
    "# print(y.shape)\n",
    "# print(data_train.data.shape)\n",
    "\n",
    "# all_y = val_loader.dataset.labels\n",
    "# all_X = val_loader.dataset.data\n",
    "# # nn_model.eval() # Evaluation mode\n",
    "# predictions = nn_model(all_X[0])\n",
    "# print(\"pred shape \", all_X.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

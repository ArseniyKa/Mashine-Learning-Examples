{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "\n",
    "прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 1.  -2.   3. ]\n",
      " [-1.   2.   0.1]]\n",
      "analytic grad is \n",
      " [[ 1.61599406  0.          2.76252524]\n",
      " [-0.         -0.5060347  -0.94302519]]\n",
      "numeric grad array is \n",
      " [[ 1.61599406  0.          2.76252524]\n",
      " [ 0.         -0.5060347  -0.94302519]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 1.  -2.   3. ]\n",
      " [-1.   2.   0.1]]\n",
      "analytic grad is \n",
      " [[-2.48170965e-04  2.14648270e-04 -6.53086067e-05]\n",
      " [ 5.30725442e-05  2.79276613e-04 -8.01671530e-05]]\n",
      "numeric grad array is \n",
      " [[-2.48170965e-04  2.14648270e-04 -6.53086067e-05]\n",
      " [ 5.30725442e-05  2.79276613e-04 -8.01671530e-05]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-1.97365551e-03  6.80635002e-04  5.55869402e-05  5.41565103e-04]\n",
      " [ 1.83213048e-04 -5.59157557e-04  8.94307167e-04  1.98126611e-04]\n",
      " [-1.28938649e-03  1.30851716e-03 -1.43047956e-03  9.00929234e-04]]\n",
      "analytic grad is \n",
      " [[-1.5033225   1.83023836  0.93577972 -0.30656704]\n",
      " [ 3.006645   -3.66047671 -1.87155943  0.61313409]\n",
      " [ 0.15033225  5.49071507 -0.09357797 -0.91970113]]\n",
      "numeric grad array is \n",
      " [[-1.5033225   1.83023836  0.93577972 -0.30656704]\n",
      " [ 3.006645   -3.66047671 -1.87155943  0.61313409]\n",
      " [ 0.15033225  5.49071507 -0.09357797 -0.91970113]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-1.33418835e-03  2.29823155e-04  1.28956322e-03  3.55224308e-05]]\n",
      "analytic grad is \n",
      " [[ 0.         -1.86802229  1.75340995 -0.53103695]]\n",
      "numeric grad array is \n",
      " [[ 0.         -1.86802229  1.75340995 -0.53103695]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 1.12196096e-03  2.60386566e-03 -7.70736619e-05]\n",
      " [-4.77813258e-04  2.19520154e-04  8.68511430e-04]\n",
      " [ 1.29594320e-03 -6.02318160e-04  1.00980449e-03]\n",
      " ...\n",
      " [ 6.03859817e-04  6.38097311e-04 -2.10330014e-03]\n",
      " [ 7.08521990e-04  4.65736852e-04  1.20636367e-03]\n",
      " [-1.15133796e-03  3.27460859e-04 -3.72324315e-04]]\n",
      "analytic grad is \n",
      " [[ 1.19667622e-05 -4.10892249e-07  3.72685554e-06]\n",
      " [ 1.42586847e-05 -4.89587985e-07  1.48172437e-06]\n",
      " [ 4.05510533e-05 -1.39236605e-06 -3.73493220e-06]\n",
      " ...\n",
      " [-1.11350711e-04  3.82335198e-06  5.25464616e-06]\n",
      " [-9.94988794e-05  3.41640601e-06  1.66032268e-06]\n",
      " [-7.92541950e-05  2.72128198e-06 -4.03205868e-06]]\n",
      "numeric grad array is \n",
      " [[ 1.19667387e-05 -4.10915746e-07  3.72684106e-06]\n",
      " [ 1.42586831e-05 -4.89652763e-07  1.48170365e-06]\n",
      " [ 4.05510736e-05 -1.39239731e-06 -3.73492348e-06]\n",
      " ...\n",
      " [-1.11350706e-04  3.82334164e-06  5.25464117e-06]\n",
      " [-9.94988536e-05  3.41637829e-06  1.66036074e-06]\n",
      " [-7.92542032e-05  2.72128986e-06 -4.03206357e-06]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-1.73255057e-03  9.10141390e-05 -1.85158888e-03]]\n",
      "analytic grad is \n",
      " [[ 7.41957880e-04 -2.54759588e-05 -4.15231296e-05]]\n",
      "numeric grad array is \n",
      " [[ 7.41957873e-04 -2.54759769e-05 -4.15231405e-05]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-0.00026905 -0.00015843 -0.00064998  0.00023561 -0.00141219 -0.0008798\n",
      "   0.00167757 -0.00057008 -0.00054801 -0.00084989]\n",
      " [ 0.00102614  0.00038763 -0.00106321  0.00024265  0.00104908  0.00060902\n",
      "   0.00085969  0.00165582 -0.00126595  0.00049468]\n",
      " [-0.00076686 -0.00081201  0.00015957 -0.00106316 -0.00017784  0.001301\n",
      "   0.00079924 -0.00132382 -0.00156246 -0.00024052]]\n",
      "analytic grad is \n",
      " [[ 0.00000000e+00  3.58212626e-04  0.00000000e+00  3.58097873e-04\n",
      "   3.58150827e-04  0.00000000e+00  3.58033924e-04  3.58710036e-04\n",
      "   0.00000000e+00 -3.22325557e-03]\n",
      " [ 0.00000000e+00  8.44649757e-05  0.00000000e+00  8.44379174e-05\n",
      "   8.44504038e-05  0.00000000e+00  8.44228387e-05  8.45822628e-05\n",
      "   0.00000000e+00 -7.60029612e-04]\n",
      " [ 0.00000000e+00  1.54189502e-03  0.00000000e+00  1.54139312e-03\n",
      "   1.54165015e-03  0.00000000e+00  1.54115335e-03  1.54402452e-03\n",
      "   0.00000000e+00 -1.38743274e-02]]\n",
      "numeric grad array is \n",
      " [[ 0.00000000e+00  3.58212615e-04  0.00000000e+00  3.58097862e-04\n",
      "   3.58150798e-04  0.00000000e+00  3.58033914e-04  3.58709995e-04\n",
      "   0.00000000e+00 -3.22325553e-03]\n",
      " [ 0.00000000e+00  8.44649684e-05  0.00000000e+00  8.44379233e-05\n",
      "   8.44504022e-05  0.00000000e+00  8.44228687e-05  8.45822079e-05\n",
      "   0.00000000e+00 -7.60029639e-04]\n",
      " [ 0.00000000e+00  1.54189506e-03  0.00000000e+00  1.54139310e-03\n",
      "   1.54165010e-03  0.00000000e+00  1.54115338e-03  1.54402455e-03\n",
      "   0.00000000e+00 -1.38743274e-02]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-6.21641435e-04  5.58878233e-04 -1.05267052e-03  2.37189377e-04\n",
      "   3.90275585e-04 -9.68625458e-05  5.29096938e-05  1.94690770e-03\n",
      "  -1.60191399e-04  8.37502194e-04]]\n",
      "analytic grad is \n",
      " [[ 0.          0.20003043  0.          0.19996583  0.19999729  0.\n",
      "   0.19993243  0.20030744  0.         -1.7999132 ]]\n",
      "numeric grad array is \n",
      " [[ 0.          0.20003043  0.          0.19996583  0.19999729  0.\n",
      "   0.19993243  0.20030744  0.         -1.7999132 ]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 5.88921615e-04 -7.83157141e-04 -1.76049658e-03]\n",
      " [ 2.89742218e-04  3.36321714e-04  1.21929277e-03]\n",
      " [-9.60196226e-04  6.31096920e-04 -1.06689282e-03]\n",
      " ...\n",
      " [-1.10271763e-03 -1.30273392e-03 -3.16518360e-05]\n",
      " [-1.37680448e-04 -3.26463798e-04  1.00704554e-03]\n",
      " [ 1.23954248e-03 -4.42987199e-04  1.32286296e-03]]\n",
      "analytic grad is \n",
      " [[ 0.01177843 -0.01566314 -0.0352268 ]\n",
      " [ 0.00579484  0.00672643  0.02437915]\n",
      " [-0.01920392  0.01262194 -0.02132095]\n",
      " ...\n",
      " [-0.02205435 -0.02605468 -0.00065682]\n",
      " [-0.00275361 -0.00652928  0.02013339]\n",
      " [ 0.02479085 -0.00885974  0.02647551]]\n",
      "numeric grad array is \n",
      " [[ 0.01177843 -0.01566314 -0.0352268 ]\n",
      " [ 0.00579484  0.00672643  0.02437915]\n",
      " [-0.01920392  0.01262194 -0.02132095]\n",
      " ...\n",
      " [-0.02205435 -0.02605468 -0.00065682]\n",
      " [-0.00275361 -0.00652928  0.02013339]\n",
      " [ 0.02479085 -0.00885974  0.02647551]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 0.001761   -0.00102397  0.0009029 ]]\n",
      "analytic grad is \n",
      " [[ 0.03521994 -0.0204794   0.018246  ]]\n",
      "numeric grad array is \n",
      " [[ 0.03521994 -0.0204794   0.018246  ]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[ 2.23898900e-04 -6.03847041e-04  5.33202441e-04 -1.06337684e-03\n",
      "   2.34740629e-04  4.71861056e-04  3.34351247e-04  5.87238551e-04\n",
      "   1.06177291e-03 -7.47296332e-04]\n",
      " [ 2.14644234e-03  4.17133274e-04 -3.35072440e-04  7.62709877e-04\n",
      "  -1.93276932e-04 -6.93161069e-04  3.99804001e-04  6.55655699e-04\n",
      "  -3.57176485e-04 -5.35623123e-04]\n",
      " [-4.14775169e-04  5.61607264e-04 -9.97950775e-04 -3.62522524e-04\n",
      "  -9.43603342e-05 -1.09712568e-03  7.27709399e-04  6.67162095e-05\n",
      "   1.70715933e-03  1.84940984e-03]]\n",
      "analytic grad is \n",
      " [[ 0.00447798 -0.01207694  0.01066405 -0.02126754  0.00469481  0.00943722\n",
      "   0.00668702  0.01174477  0.02123546 -0.01494593]\n",
      " [ 0.04292885  0.00834267 -0.00670145  0.0152542  -0.00386554 -0.01386322\n",
      "   0.00799608  0.01311311 -0.00714353 -0.01071246]\n",
      " [-0.0082955   0.01366615 -0.01995902 -0.00481622  0.00054716 -0.02194251\n",
      "   0.01455419  0.00376688  0.03657746  0.0369882 ]]\n",
      "numeric grad array is \n",
      " [[ 0.00447798 -0.01207694  0.01066405 -0.02126754  0.00469481  0.00943722\n",
      "   0.00668702  0.01174477  0.02123546 -0.01494593]\n",
      " [ 0.04292885  0.00834267 -0.00670145  0.0152542  -0.00386554 -0.01386322\n",
      "   0.00799608  0.01311311 -0.00714353 -0.01071246]\n",
      " [-0.0082955   0.01366615 -0.01995902 -0.00481622  0.00054716 -0.02194251\n",
      "   0.01455419  0.00376688  0.03657746  0.0369882 ]]\n",
      "==========================================\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "CHECK GRADIENT\n",
      "x is \n",
      " [[-0.00051657  0.00094885 -0.00027774  0.00106213  0.00111461 -0.00244688\n",
      "  -0.00222465  0.00036466  0.00103246 -0.0023248 ]]\n",
      "analytic grad is \n",
      " [[-0.01033132  0.21907717 -0.00555476  0.22136328  0.22242396 -0.04893762\n",
      "  -0.04449306  0.2072754   0.22076901 -0.04649595]]\n",
      "numeric grad array is \n",
      " [[-0.01033132  0.21907717 -0.00555476  0.22136328  0.22242396 -0.04893762\n",
      "  -0.04449306  0.2072754   0.22076901 -0.04649595]]\n",
      "==========================================\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction shape is \n",
      " (30,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06666666666666667"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.   -6.    9.  ]\n",
      " [ 1.    4.    0.01]]\n",
      "zer\n",
      " [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "Y = np.array([[1,3,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "print(X*Y)\n",
    "\n",
    "zer = np.ones((3,4))\n",
    "print(\"zer\\n\",zer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
